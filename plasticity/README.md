# Evaluating Plasticity Loss through a Teacherâ€“Student Setup on MNIST
## Project Description

[ðŸ“‘ Check the slides](https://docs.google.com/presentation/d/1bA_67PF6VYzYw1BacBRaPVGQXOaH8yPcao7MzLU4W54/edit?usp=sharing)

This project aims to evaluate **plasticity loss** in neural networks and explore possible solutions.  
We train a **Teacher Network** on the MNIST dataset, while a **Student Network** (same size and initialization) follows the teacher during training, attempting to minimize the [KL divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence) between the two.
For each teacher epoch, the student performs `N` epochs of training, where `N` can vary depending on the configuration.


## Installation

1. Navigate into the project folder:
   ```bash
   cd plasticity
   ```
2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```


## Project Structure

```tree
plasticity/
â”œâ”€â”€ plots/                          plots images from experiments
â”œâ”€â”€ differentdata.py                random data vs training data
â”œâ”€â”€ differentlearners.py            experiments with different optimizer
â”œâ”€â”€ good_teacher.py                 different setup with a better teacher
â”œâ”€â”€ linear.py                       implementation of a linear layer
â”œâ”€â”€ live_bright.py                  compares a live student (follows teacher) vs a bright student (trained from scratch)
â”œâ”€â”€ loader.py                       MNIST dataset loader
â”œâ”€â”€ model.py                        main model definitions (teacher & student)
â”œâ”€â”€ plotter.py                      helper for generating plots
â”œâ”€â”€ presets.py                      predefined neural network architectures
â””â”€â”€ requirements.txt                different setup with a better teacher
```
---
