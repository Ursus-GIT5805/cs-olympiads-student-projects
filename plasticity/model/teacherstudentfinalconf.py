import math
import jax
import optax
import loader
from linear import *
from model import Model
from model import batch_norm
from model import kl_divergence
from model import squaredmean_cost
import matplotlib.pyplot as plt
import copy
import random

def create_model(params):
    def run(params, a):
        a = feedforward_linear(params[0], a)

        x1 = a.copy()

        a = jax.nn.sigmoid(a)
        a = feedforward_linear(params[1], a)
        a = batch_norm(a)
        a = jax.nn.relu(a)

        a = feedforward_linear(params[2], a)
        a = batch_norm(a)

        a = a + x1
        a = jax.nn.relu(a)

        a = feedforward_linear(params[3], a)
        a = jax.nn.softmax(a)
        return a

    return Model.init(
        params,
        jax.jit(run),
    )

if __name__ == '__main__':
    teacher_epochs = 15
    student_epochs = 30
    student_final_epochs = teacher_epochs*student_epochs
    noise_amount_step = 40000
    batch_size = 250

    key = jax.random.PRNGKey(69420)
    params = linears_from_array([784, 100, 100, 100, 10], key=key)

    model_teacher = create_model(copy.deepcopy(params))
    model_student_along = create_model(copy.deepcopy(params))
    model_student_final = create_model(copy.deepcopy(params))

    train_teacher, train_student, test_data = loader.load_mnist_teacher_student()

    train_teacher_x, train_teacher_y = train_teacher
    train_student_x, _ = train_student
    test_x, test_y = test_data

    random_noise = jax.random.uniform(key, shape=(noise_amount_step * teacher_epochs, 784), minval=-math.sqrt(3), maxval=math.sqrt(3))

    key2 = jax.random.PRNGKey(69)
    random_noise_test = jax.random.uniform(key2, shape=(40000, 784), minval=-math.sqrt(3), maxval=math.sqrt(3))

    student_epochs_along_divergence = []
    accuracies = []

    for epoch in range(teacher_epochs):
        print("Teacher epochs {}/{}".format(epoch+1, teacher_epochs))

        print("Teacher learning")
        model_teacher.train(
            train_teacher_x, train_teacher_y,
            epochs=1, batch_size=batch_size,
            optimizer=optax.sgd(learning_rate=0.1),
            return_score=False,
            cost=squaredmean_cost,
            # evaluate=(test_x, test_y),
            seed=random.randint(0, int(1e7))
        )

        # the_key = jax.random.PRNGKey(epoch)
        # random_noise_step = random_noise[epoch*noise_amount_step:(epoch+1)*noise_amount_step]
        # print(random_noise_step.device)
        # train_student_y = model_teacher.forward(model_teacher.params, random_noise_step)
        train_student_y = model_teacher.evaluate(train_student_x)

        teacher_data = model_teacher.evaluate(random_noise_test)

        print("Live student epochs:")

        # print("Epoch: {}/{}".format(student_epoch+1, student_epochs))
        model_student_along.train(
            train_student_x, train_student_y,
            epochs=student_epochs, batch_size=batch_size,
            optimizer = optax.sgd(learning_rate=0.1),
            #return_score=True,
            #evaluate=(test_x, test_y),
        )
        along_student_acc = model_student_along.accuracy(test_x, test_y)
        accuracies.append(along_student_acc/100)

        along_student_data = model_student_along.forward(model_student_along.params, random_noise_test)
        div_stud_along_teacher = kl_divergence(q=along_student_data, p=teacher_data)
        student_epochs_along_divergence.append(div_stud_along_teacher)

        
    print("After student epochs:")

#    train_student_y_final = model_teacher.forward(model_teacher.params, random_noise)
#    model_student_final.train(
#        random_noise, train_student_y_final,
#        epochs=student_final_epochs, batch_size=batch_size,
#        optimizer = optax.sgd(learning_rate=0.1),
#        return_score=False,
#        # evaluate=(test_x, test_y),
#    )
#
    acc_train = model_teacher.accuracy(train_teacher_x, train_teacher_y)
    acc_test = model_teacher.accuracy(test_x, test_y)
    print("Accuracy teacher on training data: {}%".format(acc_train))
    print("Accuracy teacher on test data: {}%".format(acc_test))
 
    acc_train = model_student_along.accuracy(train_student_x, train_student_y)
    acc_test = model_student_along.accuracy(test_x, test_y)
    print("Accuracy live student on training data: {}%".format(acc_train))
    print("Accuracy live student on test data: {}%".format(acc_test))
#
#    acc_train = model_student_final.accuracy(train_x, train_y)
#    acc_test = model_student_final.accuracy(test_x, test_y)
#    print("Accuracy after student on training data: {}%".format(acc_train))
#    print("Accuracy after student on test data: {}%".format(acc_test))
#
#
#    teacher_data = model_teacher.forward(model_teacher.params, random_noise_test)
#    along_student_data = model_student_along.forward(model_student_along.params, random_noise_test)
#    final_student_data = model_student_final.forward(model_student_final.params, random_noise_test)
#
#    div_stud_follow_teacher = kl_divergence(q=along_student_data, p=teacher_data)
#    div_stud_final_teacher = kl_divergence(q=final_student_data, p=teacher_data)
#
#    print("Divergence of live student to teacher: {}".format(div_stud_follow_teacher))
#    print("Divergence of after student to teacher: {}".format(div_stud_final_teacher))
    print([float(x) for x in student_epochs_along_divergence])
    # plt.plot(student_epochs_along_divergence, label='Student')
    # plt.xlabel("Epochs")
    # plt.ylabel("KL Divergence")
    # plt.figtext(0, 0, "KL Divergence between teacher and student  with student having 30 epochs \n for each teacher epoch with optimizer sgd  and learning rate 0.1",fontsize=10)
    # # plt.figtext(0, 0, "Weight Magnitudefor every student epoch (sgd with 0.2 learning rate)", fontsize = 10)
    # plt.grid()
    # plt.clf()
    # # plt.legend()
    # # plt.plot(accuracies, label='accuracies')
    # plt.show()

#training normal data with spikes val = [0.0002432141191093251, 0.00010592854960123077, 5.2344978030305356e-05, 2.9902319511165842e-05, 1.9655830328701995e-05, 1.4463183106272481e-05, 1.1511619959492236e-05, 9.637841685616877e-06, 8.330911441589706e-06, 7.35105140847736e-06, 6.578316515515326e-06, 5.948273610556498e-06, 5.423125458037248e-06, 4.9791333367465995e-06, 4.6001982809684705e-06, 4.27484928877675e-06, 3.99413102059043e-06, 3.751020358322421e-06, 3.540014631653321e-06, 3.356480192451272e-06, 3.1960837532096775e-06, 3.0558651360479416e-06, 2.9327309221116593e-06, 2.8243182441656245e-06, 2.7285432224744e-06, 2.643631660248502e-06, 2.5681197257654276e-06, 2.500796654203441e-06, 2.440561274852371e-06, 2.3864324703026796e-06, 0.00020469560695346445, 8.629071817267686e-05, 4.310618169256486e-05, 2.6262230676366016e-05, 1.891873216663953e-05, 1.5202213944576215e-05, 1.2994765711482614e-05, 1.1486409675853793e-05, 1.034577508107759e-05, 9.426677934243344e-06, 8.656179488752969e-06, 7.995095984369982e-06, 7.419685061904602e-06, 6.914237474120455e-06, 6.467490038630785e-06, 6.070527888368815e-06, 5.716401119570946e-06, 5.399328529165359e-06, 5.114237410452915e-06, 4.857453404838452e-06, 4.625346264219843e-06, 4.4148719098302536e-06, 4.223399628244806e-06, 4.048731170769315e-06, 3.8888779272383545e-06, 3.7421721117425477e-06, 3.6070919122721534e-06, 3.4824645354092354e-06, 3.367340696058818e-06, 3.2606994864181615e-06, 0.00018506415653973818, 7.388801168417558e-05, 3.8937399949645624e-05, 2.6329425963922404e-05, 2.07131051865872e-05, 1.7573613149579614e-05, 1.5480209185625426e-05, 1.3917953765485436e-05, 1.2671916920226067e-05, 1.1638974683592096e-05, 1.0761038538475987e-05, 1.0002245289797429e-05, 9.338562449556775e-06, 8.75251498655416e-06, 8.231202627939638e-06, 7.764680958644021e-06, 7.345144695136696e-06, 6.9667648858739994e-06, 6.623919489356922e-06, 6.312416189757641e-06, 6.028684310876997e-06, 5.769501967733959e-06, 5.532381237571826e-06, 5.3147350627114065e-06, 5.114566647534957e-06, 4.930118393531302e-06, 4.759795956488233e-06, 4.602030003297841e-06, 4.456009264686145e-06, 4.320245807321044e-06, 0.0001530750741949305, 6.401488644769415e-05, 3.72284630429931e-05, 2.7083659006166272e-05, 2.2064523363951594e-05, 1.8988424926646985e-05, 1.683654591033701e-05, 1.5211116078717168e-05, 1.3921418030804489e-05, 1.2863378287875094e-05, 1.1973223081440665e-05, 1.121025525208097e-05, 1.0546366866037715e-05, 9.962082003767136e-06, 9.44289695326006e-06, 8.978492587630171e-06, 8.560457899875473e-06, 8.182729288819246e-06, 7.839737008907832e-06, 7.527386969741201e-06, 7.242191713885404e-06, 6.980696525715757e-06, 6.740614026057301e-06, 6.51951995678246e-06, 6.315513473964529e-06, 6.126714652054943e-06, 5.951757430011639e-06, 5.789338956674328e-06, 5.63829507882474e-06, 5.497387974173762e-06, 0.00013778610446024686, 6.1741542594973e-05, 3.755597208510153e-05, 2.7988842703052796e-05, 2.3077278456185013e-05, 1.9985072867712006e-05, 1.780746242729947e-05, 1.6183041225303896e-05, 1.4924354218237568e-05, 1.3917593605583534e-05, 1.3089635103824548e-05, 1.2390396477712784e-05, 1.178743059426779e-05, 1.1258533959335182e-05, 1.0788283361762296e-05, 1.0365710295445751e-05, 9.983133168134373e-06, 9.634691195969936e-06, 9.315720490121748e-06, 9.02257852430921e-06, 8.752285793889314e-06, 8.502186574332882e-06, 8.270310900115874e-06, 8.054823410930112e-06, 7.854336217860691e-06, 7.667135832889471e-06, 7.49239552533254e-06, 7.328608262469061e-06, 7.175023256422719e-06, 7.030843335087411e-06, 0.00012998933380004019, 5.978995613986626e-05, 3.902319076587446e-05, 3.0137707653921098e-05, 2.4965571356005967e-05, 2.1542004105867818e-05, 1.9176182831870392e-05, 1.7490279788034968e-05, 1.6242825950030237e-05, 1.5281426385627128e-05, 1.4508975255012047e-05, 1.3864249922335148e-05, 1.3309967471286654e-05, 1.2822139069612604e-05, 1.238605455000652e-05, 1.1991396604571491e-05, 1.1631050256255548e-05, 1.1300246114842594e-05, 1.0994951480824966e-05, 1.071228598448215e-05, 1.0449824003444519e-05, 1.0205673788732383e-05, 9.977870831789915e-06, 9.764919013832696e-06, 9.565789696353022e-06, 9.37917320698034e-06, 9.203879926644731e-06, 9.039174074132461e-06, 8.883856025931891e-06, 8.737363714317326e-06, 0.00012268791033420712, 6.178132025524974e-05, 4.191056723357178e-05, 3.247450149501674e-05, 2.6934909328701906e-05, 2.3401284124702215e-05, 2.1041607396909967e-05, 1.939209505508188e-05, 1.8179700418841094e-05, 1.7242708054254763e-05, 1.6484811567352153e-05, 1.5849149349378422e-05, 1.5300882296287455e-05, 1.481855088059092e-05, 1.4386920156539418e-05, 1.399753637087997e-05, 1.3642847989103757e-05, 1.3317806406121235e-05, 1.30184071167605e-05, 1.2741445971187204e-05, 1.2484840226534288e-05, 1.2246091500855982e-05, 1.2023265298921615e-05, 1.181510197056923e-05, 1.1620240002230275e-05, 1.143715599027928e-05, 1.1265170542174019e-05, 1.1103144970547874e-05, 1.0950390787911601e-05, 1.0806136742758099e-05, 0.00011876590724568814, 6.34656025795266e-05, 4.338415965321474e-05, 3.4004831832135096e-05, 2.8826507332269102e-05, 2.5641587853897363e-05, 2.352052069909405e-05, 2.200870221713558e-05, 2.0865969418082386e-05, 1.996005084947683e-05, 1.921421971928794e-05, 1.858284122135956e-05, 1.8037027984973975e-05, 1.7557838873472065e-05, 1.7131384083768353e-05, 1.6748283087508753e-05, 1.6400917957071215e-05, 1.6084324670373462e-05, 1.579404852236621e-05, 1.552650246594567e-05, 1.5278952560038306e-05, 1.5048937711981125e-05, 1.483455616835272e-05, 1.4634334547736216e-05, 1.4446593922912143e-05, 1.4270476640376728e-05, 1.4104908586887177e-05, 1.3948597370472271e-05, 1.3801234672428109e-05, 1.3661887351190671e-05, 0.00011368608102202415, 6.482104799943045e-05, 4.6057026338530704e-05, 3.728272713487968e-05, 3.2440719223814085e-05, 2.9398514016065747e-05, 2.7291025617159903e-05, 2.572584526205901e-05, 2.4503106033080257e-05, 2.3512347979703918e-05, 2.2688776880386285e-05, 2.198946640419308e-05, 2.1386518710642122e-05, 2.0859417418250814e-05, 2.039399987552315e-05, 1.9978171621914953e-05, 1.9604352928581648e-05, 1.926557524711825e-05, 1.8956778149004094e-05, 1.8673956219572574e-05, 1.8413731595501304e-05, 1.8173359421780333e-05, 1.7950185792869888e-05, 1.7742602722137235e-05, 1.754904405970592e-05, 1.7367972759529948e-05, 1.719815918477252e-05, 1.703879388514906e-05, 1.688856355031021e-05, 1.6746740584494546e-05, 0.00010792659304570407, 6.423953163903207e-05, 4.7112949687289074e-05, 3.9295791793847457e-05, 3.5091579775325954e-05, 3.245453262934461e-05, 3.059351729461923e-05, 2.917291749326978e-05, 2.803456300171092e-05, 2.709455839067232e-05, 2.6302222977392375e-05, 2.562360532465391e-05, 2.503510950191412e-05, 2.4519662474631332e-05, 2.406369094387628e-05, 2.3657239580643363e-05, 2.32924194278894e-05, 2.2962785806157626e-05, 2.2663425625069067e-05, 2.2390115191228688e-05, 2.2139231077744626e-05, 2.1907806512899697e-05, 2.1694226234103553e-05, 2.1495890905498527e-05, 2.1311449017957784e-05, 2.1139201635378413e-05, 2.0978153770556673e-05, 2.0827121261390857e-05, 2.0685309209511615e-05, 2.055173354165163e-05, 0.00010134206240763888, 6.400231359293684e-05, 4.886828173766844e-05, 4.193056156509556e-05, 3.8212361687328666e-05, 3.587328683352098e-05, 3.420257780817337e-05, 3.2907330023590475e-05, 3.185461537213996e-05, 3.0974653782323e-05, 3.0225935915950686e-05, 2.9580243790405802e-05, 2.9017619453952648e-05, 2.852335455827415e-05, 2.8085560188628733e-05, 2.7695237804437056e-05, 2.7344705813447945e-05, 2.7028048862121068e-05, 2.674058123375289e-05, 2.647825203894172e-05, 2.6237910788040608e-05, 2.6016672563855536e-05, 2.5812310923356563e-05, 2.5622737666708417e-05, 2.544663402659353e-05, 2.5282583010266535e-05, 2.5129404093604535e-05, 2.498592766642105e-05, 2.4851246053003706e-05, 2.472460619173944e-05, 9.450440120417625e-05, 6.294094782788306e-05, 4.986405110685155e-05, 4.3951073166681454e-05, 4.09110143664293e-05, 3.9079201087588444e-05, 3.7802081351401284e-05, 3.681515227071941e-05, 3.600387935875915e-05, 3.531428592395969e-05, 3.4717220842139795e-05, 3.4193919418612495e-05, 3.373204526724294e-05, 3.332165942993015e-05, 3.29548456647899e-05, 3.2625477615511045e-05, 3.232789458706975e-05, 3.205789107596502e-05, 3.1811654480407014e-05, 3.1586179829901084e-05, 3.1378691346617416e-05, 3.118703534710221e-05, 3.1009753001853824e-05, 3.0844756111036986e-05, 3.069098966079764e-05, 3.054751141462475e-05, 3.0413149943342432e-05, 3.0287294066511095e-05, 3.016900700458791e-05, 3.005767757713329e-05, 8.874142076820135e-05, 6.286885763984174e-05, 5.199633596930653e-05, 4.71207604277879e-05, 4.468232873477973e-05, 4.326268026488833e-05, 4.22939847339876e-05, 4.1544550185790285e-05, 4.091992377652787e-05, 4.0377748518949375e-05, 3.9897287933854386e-05, 3.9467653550673276e-05, 3.908095823135227e-05, 3.8731450331397355e-05, 3.8414196751546115e-05, 3.812538125202991e-05, 3.786125671467744e-05, 3.7618723581545055e-05, 3.7395100662251934e-05, 3.7188914575381204e-05, 3.6997636925661936e-05, 3.681961970869452e-05, 3.6653764254879206e-05, 3.649854625109583e-05, 3.635333268903196e-05, 3.621714131440967e-05, 3.60890626325272e-05, 3.596844544517808e-05, 3.585467129596509e-05, 3.574727816157974e-05, 8.351804717676714e-05, 6.300902168732136e-05, 5.4278385505313054e-05, 5.0369555538054556e-05, 4.844868090003729e-05, 4.735848415293731e-05, 4.6626937546534464e-05, 4.6061752073001117e-05, 4.558339060167782e-05, 4.515944965532981e-05, 4.4775704736821353e-05, 4.4425210944609717e-05, 4.4103868276579306e-05, 4.3808231566799805e-05, 4.353551412350498e-05, 4.328420618548989e-05, 4.3051011743955314e-05, 4.283450834918767e-05, 4.263294977135956e-05, 4.2444735299795866e-05, 4.226870441925712e-05, 4.210350016364828e-05, 4.194833672954701e-05, 4.180322139291093e-05, 4.166628423263319e-05, 4.153699774178676e-05, 4.141485260333866e-05, 4.1299012082163244e-05, 4.118942524655722e-05, 4.1085433622356504e-05, 8.059466927079484e-05, 6.416918040486053e-05, 5.716636951547116e-05, 5.412997052189894e-05, 5.275150033412501e-05, 5.2052819228265435e-05, 5.162596426089294e-05, 5.1305254601174966e-05, 5.102530849399045e-05, 5.076226079836488e-05, 5.050848994869739e-05, 5.0263242883374915e-05, 5.002651960239746e-05, 4.9799382395576686e-05, 4.9582478823140264e-05, 4.9375179514754564e-05, 4.917925616609864e-05, 4.8992813390213996e-05, 4.881632776232436e-05, 4.8648718802724034e-05, 4.8489913751836866e-05, 4.8339330533053726e-05, 4.819628520635888e-05, 4.8060494009405375e-05, 4.7931775043252856e-05, 4.7809324314584956e-05, 4.769279985339381e-05, 4.758195427712053e-05, 4.747628918266855e-05, 4.737531708087772e-05]
# training normal data withous spikes val = 

