# import math
# import jax
# import optax
# import loader
# from linear import *
# from model import Model
# from model import batch_norm
# from model import kl_divergence
# from model import squaredmean_cost
# import matplotlib.pyplot as plt
# import copy
# import random

# def create_model(params):
#     def run(params, a):
#         a = feedforward_linear(params[0], a)

#         x1 = a.copy()

#         a = jax.nn.sigmoid(a)
#         a = feedforward_linear(params[1], a)
#         a = batch_norm(a)
#         a = jax.nn.relu(a)

#         a = feedforward_linear(params[2], a)
#         a = batch_norm(a)

#         a = a + x1
#         a = jax.nn.relu(a)

#         a = feedforward_linear(params[3], a)
#         a = jax.nn.softmax(a)
#         return a

#     return Model.init(
#         params,
#         jax.jit(run),
#     )

# if __name__ == '__main__':
#     teacher_epochs = 15
#     student_epochs = 30
#     student_final_epochs = teacher_epochs*student_epochs
#     noise_amount_step = 40000
#     batch_size = 250

#     key = jax.random.PRNGKey(69420)
#     params = linears_from_array([784, 100, 100, 100, 10], key=key)

#     model_teacher = create_model(copy.deepcopy(params))
#     model_student_along = create_model(copy.deepcopy(params))
#     model_student_final = create_model(copy.deepcopy(params))

#     train_teacher, train_student, test_data = loader.load_mnist_teacher_student()

#     train_teacher_x, train_teacher_y = train_teacher
#     train_student_x, _ = train_student
#     test_x, test_y = test_data

#     random_noise = jax.random.uniform(key, shape=(noise_amount_step * teacher_epochs, 784), minval=-math.sqrt(3), maxval=math.sqrt(3))

#     key2 = jax.random.PRNGKey(69)
#     random_noise_test = jax.random.uniform(key2, shape=(40000, 784), minval=-math.sqrt(3), maxval=math.sqrt(3))

#     student_epochs_along_divergence = []
#     accuracies = []

#     for epoch in range(teacher_epochs):
#         print("Teacher epochs {}/{}".format(epoch+1, teacher_epochs))

#         print("Teacher learning")
#         model_teacher.train(
#             train_teacher_x, train_teacher_y,
#             epochs=1, batch_size=batch_size,
#             optimizer=optax.sgd(learning_rate=0.1),
#             return_score=False,
#             cost=squaredmean_cost,
#             # evaluate=(test_x, test_y),
#             seed=random.randint(0, int(1e7))
#         )

#         # the_key = jax.random.PRNGKey(epoch)
#         # random_noise_step = random_noise[epoch*noise_amount_step:(epoch+1)*noise_amount_step]
#         # print(random_noise_step.device)
#         # train_student_y = model_teacher.forward(model_teacher.params, random_noise_step)
#         train_student_y = model_teacher.evaluate(train_student_x)

#         teacher_data = model_teacher.evaluate(random_noise_test)

#         print("Live student epochs:")
#         for student_epoch in range(student_epochs):
#             print("Epoch: {}/{}".format(student_epoch+1, student_epochs))
#             model_student_along.train(
#                 train_student_x, train_student_y,
#                 epochs=1, batch_size=batch_size,
#                 optimizer = optax.sgd(learning_rate=0.1),
#                 #return_score=True,
#                 #evaluate=(test_x, test_y),
#             )
#             along_student_acc = model_student_along.accuracy(test_x, test_y)
#             accuracies.append(along_student_acc/100)

#             along_student_data = model_student_along.forward(model_student_along.params, random_noise_test)
#             div_stud_along_teacher = kl_divergence(q=along_student_data, p=teacher_data)
#             student_epochs_along_divergence.append(div_stud_along_teacher)

        
#     print("After student epochs:")

# #    train_student_y_final = model_teacher.forward(model_teacher.params, random_noise)
# #    model_student_final.train(
# #        random_noise, train_student_y_final,
# #        epochs=student_final_epochs, batch_size=batch_size,
# #        optimizer = optax.sgd(learning_rate=0.1),
# #        return_score=False,
# #        # evaluate=(test_x, test_y),
# #    )
# #
#     acc_train = model_teacher.accuracy(train_teacher_x, train_teacher_y)
#     acc_test = model_teacher.accuracy(test_x, test_y)
#     print("Accuracy teacher on training data: {}%".format(acc_train))
#     print("Accuracy teacher on test data: {}%".format(acc_test))
 
#     acc_train = model_student_along.accuracy(train_student_x, train_student_y)
#     acc_test = model_student_along.accuracy(test_x, test_y)
#     print("Accuracy live student on training data: {}%".format(acc_train))
#     print("Accuracy live student on test data: {}%".format(acc_test))
# #
# #    acc_train = model_student_final.accuracy(train_x, train_y)
# #    acc_test = model_student_final.accuracy(test_x, test_y)
# #    print("Accuracy after student on training data: {}%".format(acc_train))
# #    print("Accuracy after student on test data: {}%".format(acc_test))
# #
# #
# #    teacher_data = model_teacher.forward(model_teacher.params, random_noise_test)
# #    along_student_data = model_student_along.forward(model_student_along.params, random_noise_test)
# #    final_student_data = model_student_final.forward(model_student_final.params, random_noise_test)
# #
# #    div_stud_follow_teacher = kl_divergence(q=along_student_data, p=teacher_data)
# #    div_stud_final_teacher = kl_divergence(q=final_student_data, p=teacher_data)
# #
# #    print("Divergence of live student to teacher: {}".format(div_stud_follow_teacher))
# #    print("Divergence of after student to teacher: {}".format(div_stud_final_teacher))
#     print([float(x) for x in student_epochs_along_divergence])
#     # plt.plot(student_epochs_along_divergence, label='Student')
#     # plt.xlabel("Epochs")
#     # plt.ylabel("KL Divergence")
#     # plt.figtext(0, 0, "KL Divergence between teacher and student  with student having 30 epochs \n for each teacher epoch with optimizer sgd  and learning rate 0.1",fontsize=10)
#     # # plt.figtext(0, 0, "Weight Magnitudefor every student epoch (sgd with 0.2 learning rate)", fontsize = 10)
#     # plt.grid()
#     # plt.clf()
#     # # plt.legend()
#     # # plt.plot(accuracies, label='accuracies')
#     # plt.show()

import math
import jax
import optax

import loader
from linear import *
from model import Model
from model import batch_norm
from model import kl_divergence
from model import squaredmean_cost
import presets

import matplotlib.pyplot as plt
import copy
import random

if __name__ == '__main__':
    teacher_epochs = 15
    student_epochs = 30
    student_final_epochs = teacher_epochs*student_epochs
    noise_amount_step = 40000
    batch_size = 250

    key = jax.random.PRNGKey(69420)

    model_teacher = presets.Resnet1_mnist(key)
    model_student_along = presets.Resnet1_mnist(key)
    model_student_final = presets.Resnet1_mnist(key)

    train_data, test_data = loader.load_mnist_raw()


    train_teacher_x, train_teacher_y = train_data
    # train_student_x, _ = train_student
    test_x, test_y = test_data
    random_noise = jax.random.uniform(key, shape=(noise_amount_step * teacher_epochs, 784), minval=-math.sqrt(3), maxval=math.sqrt(3))

    key2 = jax.random.PRNGKey(69)
    random_noise_test = jax.random.uniform(key2, shape=(40000, 784), minval=-math.sqrt(3), maxval=math.sqrt(3))

    student_epochs_along_divergence = []
    accuracies = []

    for epoch in range(teacher_epochs):
        print("Teacher epochs {}/{}".format(epoch+1, teacher_epochs))

        print("Teacher learning")
        model_teacher.train(
            train_teacher_x, train_teacher_y,
            epochs=1, batch_size=batch_size,
            optimizer=optax.sgd(learning_rate=0.1),
            return_score=False,
            cost=squaredmean_cost,
            # evaluate=(test_x, test_y),
            seed=random.randint(0, int(1e7))
        )

        # the_key = jax.random.PRNGKey(epoch)
        random_noise_step = random_noise[epoch*noise_amount_step:(epoch+1)*noise_amount_step]
        print(random_noise_step.device)
        train_student_y = model_teacher.forward(model_teacher.params, random_noise_step)

        teacher_data = model_teacher.forward(model_teacher.params, random_noise_test)

        print("Live student epochs:")
        # for student_epoch in range(student_epochs):
        # print("Epoch: {}/{}".format(student_epoch+1, student_epochs))
        bright_student = presets.Resnet1_mnist(key)

        bright_student.train(
            random_noise_step, train_student_y,
            epochs=student_epochs*(epoch+1), batch_size=batch_size,
            optimizer = optax.sgd(learning_rate=0.1),
            #return_score=True,
            #evaluate=(test_x, test_y),
        )
        along_student_acc = bright_student.accuracy(test_x, test_y)
        accuracies.append(along_student_acc/100)

        along_student_data = bright_student.forward(bright_student.params, random_noise_test)
        div_stud_along_teacher = kl_divergence(q=along_student_data, p=teacher_data)
        student_epochs_along_divergence.append(div_stud_along_teacher)


    print("After student epochs:")

#    train_student_y_final = model_teacher.forward(model_teacher.params, random_noise)
#    model_student_final.train(
#        random_noise, train_student_y_final,
#        epochs=student_final_epochs, batch_size=batch_size,
#        optimizer = optax.sgd(learning_rate=0.1),
#        return_score=False,
#        # evaluate=(test_x, test_y),
#    )
#
    acc_train = model_teacher.accuracy(train_teacher_x, train_teacher_y)
    acc_test = model_teacher.accuracy(test_x, test_y)
    print("Accuracy teacher on training data: {}%".format(acc_train))
    print("Accuracy teacher on test data: {}%".format(acc_test))

    acc_train = model_student_along.accuracy(train_teacher_x, train_teacher_y)
    acc_test = model_student_along.accuracy(test_x, test_y)
    print("Accuracy live student on training data: {}%".format(acc_train))
    print("Accuracy live student on test data: {}%".format(acc_test))
    print([float(x) for x in student_epochs_along_divergence])
#
#    acc_train = model_student_final.accuracy(train_x, train_y)
#    acc_test = model_student_final.accuracy(test_x, test_y)
#    print("Accuracy after student on training data: {}%".format(acc_train))
#    print("Accuracy after student on test data: {}%".format(acc_test))
#
#
#    teacher_data = model_teacher.forward(model_teacher.params, random_noise_test)
#    along_student_data = model_student_along.forward(model_student_along.params, random_noise_test)
#    final_student_data = model_student_final.forward(model_student_final.params, random_noise_test)
#
#    div_stud_follow_teacher = kl_divergence(q=along_student_data, p=teacher_data)
#    div_stud_final_teacher = kl_divergence(q=final_student_data, p=teacher_data)
#
#    print("Divergence of live student to teacher: {}".format(div_stud_follow_teacher))
#    print("Divergence of after student to teacher: {}".format(div_stud_final_teacher))

    # plt.plot(student_epochs_along_divergence, label='divergence')
    # plt.show()
    # plt.clf()
    # plt.plot(accuracies, label='accuracies')
    # plt.show()

#training normal data with spikes val = [0.0002432141191093251, 0.00010592854960123077, 5.2344978030305356e-05, 2.9902319511165842e-05, 1.9655830328701995e-05, 1.4463183106272481e-05, 1.1511619959492236e-05, 9.637841685616877e-06, 8.330911441589706e-06, 7.35105140847736e-06, 6.578316515515326e-06, 5.948273610556498e-06, 5.423125458037248e-06, 4.9791333367465995e-06, 4.6001982809684705e-06, 4.27484928877675e-06, 3.99413102059043e-06, 3.751020358322421e-06, 3.540014631653321e-06, 3.356480192451272e-06, 3.1960837532096775e-06, 3.0558651360479416e-06, 2.9327309221116593e-06, 2.8243182441656245e-06, 2.7285432224744e-06, 2.643631660248502e-06, 2.5681197257654276e-06, 2.500796654203441e-06, 2.440561274852371e-06, 2.3864324703026796e-06, 0.00020469560695346445, 8.629071817267686e-05, 4.310618169256486e-05, 2.6262230676366016e-05, 1.891873216663953e-05, 1.5202213944576215e-05, 1.2994765711482614e-05, 1.1486409675853793e-05, 1.034577508107759e-05, 9.426677934243344e-06, 8.656179488752969e-06, 7.995095984369982e-06, 7.419685061904602e-06, 6.914237474120455e-06, 6.467490038630785e-06, 6.070527888368815e-06, 5.716401119570946e-06, 5.399328529165359e-06, 5.114237410452915e-06, 4.857453404838452e-06, 4.625346264219843e-06, 4.4148719098302536e-06, 4.223399628244806e-06, 4.048731170769315e-06, 3.8888779272383545e-06, 3.7421721117425477e-06, 3.6070919122721534e-06, 3.4824645354092354e-06, 3.367340696058818e-06, 3.2606994864181615e-06, 0.00018506415653973818, 7.388801168417558e-05, 3.8937399949645624e-05, 2.6329425963922404e-05, 2.07131051865872e-05, 1.7573613149579614e-05, 1.5480209185625426e-05, 1.3917953765485436e-05, 1.2671916920226067e-05, 1.1638974683592096e-05, 1.0761038538475987e-05, 1.0002245289797429e-05, 9.338562449556775e-06, 8.75251498655416e-06, 8.231202627939638e-06, 7.764680958644021e-06, 7.345144695136696e-06, 6.9667648858739994e-06, 6.623919489356922e-06, 6.312416189757641e-06, 6.028684310876997e-06, 5.769501967733959e-06, 5.532381237571826e-06, 5.3147350627114065e-06, 5.114566647534957e-06, 4.930118393531302e-06, 4.759795956488233e-06, 4.602030003297841e-06, 4.456009264686145e-06, 4.320245807321044e-06, 0.0001530750741949305, 6.401488644769415e-05, 3.72284630429931e-05, 2.7083659006166272e-05, 2.2064523363951594e-05, 1.8988424926646985e-05, 1.683654591033701e-05, 1.5211116078717168e-05, 1.3921418030804489e-05, 1.2863378287875094e-05, 1.1973223081440665e-05, 1.121025525208097e-05, 1.0546366866037715e-05, 9.962082003767136e-06, 9.44289695326006e-06, 8.978492587630171e-06, 8.560457899875473e-06, 8.182729288819246e-06, 7.839737008907832e-06, 7.527386969741201e-06, 7.242191713885404e-06, 6.980696525715757e-06, 6.740614026057301e-06, 6.51951995678246e-06, 6.315513473964529e-06, 6.126714652054943e-06, 5.951757430011639e-06, 5.789338956674328e-06, 5.63829507882474e-06, 5.497387974173762e-06, 0.00013778610446024686, 6.1741542594973e-05, 3.755597208510153e-05, 2.7988842703052796e-05, 2.3077278456185013e-05, 1.9985072867712006e-05, 1.780746242729947e-05, 1.6183041225303896e-05, 1.4924354218237568e-05, 1.3917593605583534e-05, 1.3089635103824548e-05, 1.2390396477712784e-05, 1.178743059426779e-05, 1.1258533959335182e-05, 1.0788283361762296e-05, 1.0365710295445751e-05, 9.983133168134373e-06, 9.634691195969936e-06, 9.315720490121748e-06, 9.02257852430921e-06, 8.752285793889314e-06, 8.502186574332882e-06, 8.270310900115874e-06, 8.054823410930112e-06, 7.854336217860691e-06, 7.667135832889471e-06, 7.49239552533254e-06, 7.328608262469061e-06, 7.175023256422719e-06, 7.030843335087411e-06, 0.00012998933380004019, 5.978995613986626e-05, 3.902319076587446e-05, 3.0137707653921098e-05, 2.4965571356005967e-05, 2.1542004105867818e-05, 1.9176182831870392e-05, 1.7490279788034968e-05, 1.6242825950030237e-05, 1.5281426385627128e-05, 1.4508975255012047e-05, 1.3864249922335148e-05, 1.3309967471286654e-05, 1.2822139069612604e-05, 1.238605455000652e-05, 1.1991396604571491e-05, 1.1631050256255548e-05, 1.1300246114842594e-05, 1.0994951480824966e-05, 1.071228598448215e-05, 1.0449824003444519e-05, 1.0205673788732383e-05, 9.977870831789915e-06, 9.764919013832696e-06, 9.565789696353022e-06, 9.37917320698034e-06, 9.203879926644731e-06, 9.039174074132461e-06, 8.883856025931891e-06, 8.737363714317326e-06, 0.00012268791033420712, 6.178132025524974e-05, 4.191056723357178e-05, 3.247450149501674e-05, 2.6934909328701906e-05, 2.3401284124702215e-05, 2.1041607396909967e-05, 1.939209505508188e-05, 1.8179700418841094e-05, 1.7242708054254763e-05, 1.6484811567352153e-05, 1.5849149349378422e-05, 1.5300882296287455e-05, 1.481855088059092e-05, 1.4386920156539418e-05, 1.399753637087997e-05, 1.3642847989103757e-05, 1.3317806406121235e-05, 1.30184071167605e-05, 1.2741445971187204e-05, 1.2484840226534288e-05, 1.2246091500855982e-05, 1.2023265298921615e-05, 1.181510197056923e-05, 1.1620240002230275e-05, 1.143715599027928e-05, 1.1265170542174019e-05, 1.1103144970547874e-05, 1.0950390787911601e-05, 1.0806136742758099e-05, 0.00011876590724568814, 6.34656025795266e-05, 4.338415965321474e-05, 3.4004831832135096e-05, 2.8826507332269102e-05, 2.5641587853897363e-05, 2.352052069909405e-05, 2.200870221713558e-05, 2.0865969418082386e-05, 1.996005084947683e-05, 1.921421971928794e-05, 1.858284122135956e-05, 1.8037027984973975e-05, 1.7557838873472065e-05, 1.7131384083768353e-05, 1.6748283087508753e-05, 1.6400917957071215e-05, 1.6084324670373462e-05, 1.579404852236621e-05, 1.552650246594567e-05, 1.5278952560038306e-05, 1.5048937711981125e-05, 1.483455616835272e-05, 1.4634334547736216e-05, 1.4446593922912143e-05, 1.4270476640376728e-05, 1.4104908586887177e-05, 1.3948597370472271e-05, 1.3801234672428109e-05, 1.3661887351190671e-05, 0.00011368608102202415, 6.482104799943045e-05, 4.6057026338530704e-05, 3.728272713487968e-05, 3.2440719223814085e-05, 2.9398514016065747e-05, 2.7291025617159903e-05, 2.572584526205901e-05, 2.4503106033080257e-05, 2.3512347979703918e-05, 2.2688776880386285e-05, 2.198946640419308e-05, 2.1386518710642122e-05, 2.0859417418250814e-05, 2.039399987552315e-05, 1.9978171621914953e-05, 1.9604352928581648e-05, 1.926557524711825e-05, 1.8956778149004094e-05, 1.8673956219572574e-05, 1.8413731595501304e-05, 1.8173359421780333e-05, 1.7950185792869888e-05, 1.7742602722137235e-05, 1.754904405970592e-05, 1.7367972759529948e-05, 1.719815918477252e-05, 1.703879388514906e-05, 1.688856355031021e-05, 1.6746740584494546e-05, 0.00010792659304570407, 6.423953163903207e-05, 4.7112949687289074e-05, 3.9295791793847457e-05, 3.5091579775325954e-05, 3.245453262934461e-05, 3.059351729461923e-05, 2.917291749326978e-05, 2.803456300171092e-05, 2.709455839067232e-05, 2.6302222977392375e-05, 2.562360532465391e-05, 2.503510950191412e-05, 2.4519662474631332e-05, 2.406369094387628e-05, 2.3657239580643363e-05, 2.32924194278894e-05, 2.2962785806157626e-05, 2.2663425625069067e-05, 2.2390115191228688e-05, 2.2139231077744626e-05, 2.1907806512899697e-05, 2.1694226234103553e-05, 2.1495890905498527e-05, 2.1311449017957784e-05, 2.1139201635378413e-05, 2.0978153770556673e-05, 2.0827121261390857e-05, 2.0685309209511615e-05, 2.055173354165163e-05, 0.00010134206240763888, 6.400231359293684e-05, 4.886828173766844e-05, 4.193056156509556e-05, 3.8212361687328666e-05, 3.587328683352098e-05, 3.420257780817337e-05, 3.2907330023590475e-05, 3.185461537213996e-05, 3.0974653782323e-05, 3.0225935915950686e-05, 2.9580243790405802e-05, 2.9017619453952648e-05, 2.852335455827415e-05, 2.8085560188628733e-05, 2.7695237804437056e-05, 2.7344705813447945e-05, 2.7028048862121068e-05, 2.674058123375289e-05, 2.647825203894172e-05, 2.6237910788040608e-05, 2.6016672563855536e-05, 2.5812310923356563e-05, 2.5622737666708417e-05, 2.544663402659353e-05, 2.5282583010266535e-05, 2.5129404093604535e-05, 2.498592766642105e-05, 2.4851246053003706e-05, 2.472460619173944e-05, 9.450440120417625e-05, 6.294094782788306e-05, 4.986405110685155e-05, 4.3951073166681454e-05, 4.09110143664293e-05, 3.9079201087588444e-05, 3.7802081351401284e-05, 3.681515227071941e-05, 3.600387935875915e-05, 3.531428592395969e-05, 3.4717220842139795e-05, 3.4193919418612495e-05, 3.373204526724294e-05, 3.332165942993015e-05, 3.29548456647899e-05, 3.2625477615511045e-05, 3.232789458706975e-05, 3.205789107596502e-05, 3.1811654480407014e-05, 3.1586179829901084e-05, 3.1378691346617416e-05, 3.118703534710221e-05, 3.1009753001853824e-05, 3.0844756111036986e-05, 3.069098966079764e-05, 3.054751141462475e-05, 3.0413149943342432e-05, 3.0287294066511095e-05, 3.016900700458791e-05, 3.005767757713329e-05, 8.874142076820135e-05, 6.286885763984174e-05, 5.199633596930653e-05, 4.71207604277879e-05, 4.468232873477973e-05, 4.326268026488833e-05, 4.22939847339876e-05, 4.1544550185790285e-05, 4.091992377652787e-05, 4.0377748518949375e-05, 3.9897287933854386e-05, 3.9467653550673276e-05, 3.908095823135227e-05, 3.8731450331397355e-05, 3.8414196751546115e-05, 3.812538125202991e-05, 3.786125671467744e-05, 3.7618723581545055e-05, 3.7395100662251934e-05, 3.7188914575381204e-05, 3.6997636925661936e-05, 3.681961970869452e-05, 3.6653764254879206e-05, 3.649854625109583e-05, 3.635333268903196e-05, 3.621714131440967e-05, 3.60890626325272e-05, 3.596844544517808e-05, 3.585467129596509e-05, 3.574727816157974e-05, 8.351804717676714e-05, 6.300902168732136e-05, 5.4278385505313054e-05, 5.0369555538054556e-05, 4.844868090003729e-05, 4.735848415293731e-05, 4.6626937546534464e-05, 4.6061752073001117e-05, 4.558339060167782e-05, 4.515944965532981e-05, 4.4775704736821353e-05, 4.4425210944609717e-05, 4.4103868276579306e-05, 4.3808231566799805e-05, 4.353551412350498e-05, 4.328420618548989e-05, 4.3051011743955314e-05, 4.283450834918767e-05, 4.263294977135956e-05, 4.2444735299795866e-05, 4.226870441925712e-05, 4.210350016364828e-05, 4.194833672954701e-05, 4.180322139291093e-05, 4.166628423263319e-05, 4.153699774178676e-05, 4.141485260333866e-05, 4.1299012082163244e-05, 4.118942524655722e-05, 4.1085433622356504e-05, 8.059466927079484e-05, 6.416918040486053e-05, 5.716636951547116e-05, 5.412997052189894e-05, 5.275150033412501e-05, 5.2052819228265435e-05, 5.162596426089294e-05, 5.1305254601174966e-05, 5.102530849399045e-05, 5.076226079836488e-05, 5.050848994869739e-05, 5.0263242883374915e-05, 5.002651960239746e-05, 4.9799382395576686e-05, 4.9582478823140264e-05, 4.9375179514754564e-05, 4.917925616609864e-05, 4.8992813390213996e-05, 4.881632776232436e-05, 4.8648718802724034e-05, 4.8489913751836866e-05, 4.8339330533053726e-05, 4.819628520635888e-05, 4.8060494009405375e-05, 4.7931775043252856e-05, 4.7809324314584956e-05, 4.769279985339381e-05, 4.758195427712053e-05, 4.747628918266855e-05, 4.737531708087772e-05]
# training normal data withous spikes val = [2.4058008420979604e-06, 3.2702553198760143e-06, 4.40801659351564e-06, 5.575433988269651e-06, 7.055031346681062e-06, 8.959182196122129e-06, 1.1283373169135302e-05, 1.4125247616902925e-05, 1.744047949614469e-05, 2.135098839062266e-05, 2.5925533918780275e-05, 3.116520383628085e-05, 3.675016341730952e-05, 4.260058267391287e-05, 4.881149288848974e-05]
# training random data with spikes val = [0.00010494131129235029, 7.489354175049812e-05, 5.625674020848237e-05, 4.38210554420948e-05, 3.505236600176431e-05, 2.867498369596433e-05, 2.3962165869306773e-05, 2.0352523279143497e-05, 1.7563250366947614e-05, 1.540385164844338e-05, 1.3677482456841972e-05, 1.2259589311724994e-05, 1.1063717465731315e-05, 1.0137221579498146e-05, 9.382933967572171e-06, 8.675025128468405e-06, 8.06847310741432e-06, 7.624596037203446e-06, 7.185125014075311e-06, 6.868663149361964e-06, 6.574976850970415e-06, 6.3412103372684214e-06, 6.127719643700402e-06, 5.928044174652314e-06, 5.7678125813254155e-06, 5.619100647891173e-06, 5.481102107296465e-06, 5.380175025493372e-06, 5.287598014547257e-06, 5.142030204297043e-06, 0.00011598628771025687, 8.268863166449592e-05, 6.241203664103523e-05, 4.896377504337579e-05, 3.960085450671613e-05, 3.280649252701551e-05, 2.7681970095727593e-05, 2.378073077125009e-05, 2.0705327187897637e-05, 1.835083276091609e-05, 1.6335110558429733e-05, 1.475737735745497e-05, 1.344072097708704e-05, 1.2282793250051327e-05, 1.128565509134205e-05, 1.0504133570066188e-05, 9.75080365606118e-06, 9.213843441102654e-06, 8.712998351256829e-06, 8.262909432232846e-06, 7.84706116974121e-06, 7.49559148971457e-06, 7.172568984969985e-06, 6.8751587605220266e-06, 6.593624220840866e-06, 6.3781271819607355e-06, 6.222530373634072e-06, 6.039610980224097e-06, 5.940869868936716e-06, 5.749754564021714e-06, 0.00014765339437872171, 0.00010319795546820387, 7.699298294028267e-05, 6.022189336363226e-05, 4.891295247944072e-05, 4.0688097215024754e-05, 3.453574754530564e-05, 2.982342630275525e-05, 2.6071504180436023e-05, 2.318467159057036e-05, 2.074416624964215e-05, 1.8809523680829443e-05, 1.7104088328778744e-05, 1.571995198901277e-05, 1.4475691386905964e-05, 1.349982267129235e-05, 1.2547062397061381e-05, 1.1794973033829592e-05, 1.1135642125736922e-05, 1.052951211022446e-05, 1.0032479622168466e-05, 9.573961506248452e-06, 9.178338586934842e-06, 8.859711670083925e-06, 8.387043635593727e-06, 8.080806765065063e-06, 7.907015969976783e-06, 7.588010248582577e-06, 7.3885207712010015e-06, 7.176889539550757e-06, 0.00016936085012275726, 0.00011796892067650333, 8.904449350666255e-05, 7.075511530274525e-05, 5.823381434311159e-05, 4.9158319598063827e-05, 4.232528226566501e-05, 3.7021854950580746e-05, 3.275487688370049e-05, 2.9269527658470906e-05, 2.6487889044801705e-05, 2.401370875304565e-05, 2.2120300855021924e-05, 2.038059028564021e-05, 1.8875371097237803e-05, 1.7596645193407312e-05, 1.6534315363969654e-05, 1.5484110917896032e-05, 1.466687535867095e-05, 1.395405524817761e-05, 1.3290874449012335e-05, 1.2704024811682757e-05, 1.2138211786805186e-05, 1.1695119610521942e-05, 1.114381302613765e-05, 1.0738958735601045e-05, 1.0292355909768958e-05, 9.967553523893002e-06, 9.70261316979304e-06, 9.45801457419293e-06, 0.00019532567239366472, 0.00013907103857491165, 0.00010667525930330157, 8.579710993217304e-05, 7.111603918019682e-05, 6.030082658980973e-05, 5.215144483372569e-05, 4.581209213938564e-05, 4.0585811802884564e-05, 3.635879693320021e-05, 3.289380401838571e-05, 2.9947561415610835e-05, 2.7465781386126764e-05, 2.54303067777073e-05, 2.353397576371208e-05, 2.20252823055489e-05, 2.0665733245550655e-05, 1.9427481674938463e-05, 1.8341350369155407e-05, 1.739961044222582e-05, 1.6460477127111517e-05, 1.5772848200867884e-05, 1.5149294995353557e-05, 1.4573455700883642e-05, 1.3993508218845818e-05, 1.3471300007950049e-05, 1.3029472938796971e-05, 1.2573161257023457e-05, 1.2231862456246745e-05, 1.187947145808721e-05, 0.0002083990111714229, 0.0001516139745945111, 0.00011769910634029657, 9.547155787004158e-05, 7.955293403938413e-05, 6.782305717933923e-05, 5.877018702449277e-05, 5.1700990297831595e-05, 4.598018495016731e-05, 4.1442650399403647e-05, 3.7497662560781464e-05, 3.4253225749125704e-05, 3.1663774279877543e-05, 2.933640826086048e-05, 2.7312091333442368e-05, 2.5533399821142666e-05, 2.3966656954144128e-05, 2.2632526452071033e-05, 2.1449632185976952e-05, 2.038706952589564e-05, 1.93455252883723e-05, 1.853321009548381e-05, 1.7781496353563853e-05, 1.7124568330473267e-05, 1.6511599824298173e-05, 1.5979949239408597e-05, 1.550200249766931e-05, 1.5075444935064297e-05, 1.4670860764454119e-05, 1.431850887456676e-05, 0.00019059288024436682, 0.00013959911302663386, 0.000108649670437444, 8.819051436148584e-05, 7.362566975643858e-05, 6.281134847085923e-05, 5.4671429097652435e-05, 4.833583807339892e-05, 4.321945016272366e-05, 3.9086578908609226e-05, 3.5784716601483524e-05, 3.278518852312118e-05, 3.0325600164360367e-05, 2.8271731935092248e-05, 2.6608498956193216e-05, 2.5093946533161215e-05, 2.375058647885453e-05, 2.2493260985356756e-05, 2.132451299985405e-05, 2.0428527932381257e-05, 1.9630837414297275e-05, 1.891940519271884e-05, 1.825497383833863e-05, 1.7671696696197614e-05, 1.7181468138005584e-05, 1.658775727264583e-05, 1.612500818737317e-05, 1.5808582247700542e-05, 1.5451067156391218e-05, 1.5119185263756663e-05, 0.00015646711108274758, 0.0001137118597398512, 8.813618478598073e-05, 7.132074097171426e-05, 5.959564077784307e-05, 5.116890315548517e-05, 4.47705460828729e-05, 3.9890961488708854e-05, 3.595183079596609e-05, 3.2941516110440716e-05, 3.027881757589057e-05, 2.8370946893119253e-05, 2.6635198082658462e-05, 2.5016493964358233e-05, 2.379206125624478e-05, 2.2626380086876452e-05, 2.1697716874768957e-05, 2.0798966943402775e-05, 2.01166003535036e-05, 1.937617162184324e-05, 1.8797414668370038e-05, 1.825601066229865e-05, 1.776581302692648e-05, 1.731464362819679e-05, 1.701224573480431e-05, 1.6663421774865128e-05, 1.63362728926586e-05, 1.58961283887038e-05, 1.5756268112454563e-05, 1.552476896904409e-05, 0.00012900246656499803, 9.326165309175849e-05, 7.21350297681056e-05, 5.844550469191745e-05, 4.914768214803189e-05, 4.2477397073525935e-05, 3.755509533220902e-05, 3.3823664125520736e-05, 3.0773582693655044e-05, 2.8462050977395847e-05, 2.667439366632607e-05, 2.5085537345148623e-05, 2.3890770535217598e-05, 2.2793739844928496e-05, 2.1867201212444343e-05, 2.1023866793257184e-05, 2.0344590666354634e-05, 1.97783865587553e-05, 1.9230967154726386e-05, 1.8719756553764455e-05, 1.830284236348234e-05, 1.7931470210896805e-05, 1.7552720237290487e-05, 1.7323467545793392e-05, 1.692460682534147e-05, 1.668127515586093e-05, 1.6311169019900262e-05, 1.6148547729244456e-05, 1.5969877495081164e-05, 1.5759002053528093e-05, 0.00010566347191343084, 7.56851295591332e-05, 5.84168046771083e-05, 4.758401701110415e-05, 4.0487488149665296e-05, 3.5420904168859124e-05, 3.1908435630612075e-05, 2.9028878998360597e-05, 2.7047613912145607e-05, 2.5372572054038756e-05, 2.4092440071399324e-05, 2.3084548956830986e-05, 2.211096762039233e-05, 2.1451138309203088e-05, 2.0850138753303327e-05, 2.03794570552418e-05, 1.9793726096395403e-05, 1.917966983455699e-05, 1.883696131699253e-05, 1.8523236576584168e-05, 1.8132517652702518e-05, 1.7954263967112638e-05, 1.7655313058639877e-05, 1.7443877368350513e-05, 1.719880492601078e-05, 1.700558641459793e-05, 1.678101034485735e-05, 1.6672829588060267e-05, 1.649055047892034e-05, 1.6386682545999065e-05, 8.707368397153914e-05, 6.229011341929436e-05, 4.834527862840332e-05, 3.973521961597726e-05, 3.425040631555021e-05, 3.041183481400367e-05, 2.7904685339308344e-05, 2.5807314159465022e-05, 2.430370295769535e-05, 2.3182110453490168e-05, 2.2188314687809907e-05, 2.141310142178554e-05, 2.0810231944778934e-05, 2.0267207219148986e-05, 1.9841701941913925e-05, 1.952630736923311e-05, 1.9104032617178746e-05, 1.8857674149330705e-05, 1.867893479357008e-05, 1.843036625359673e-05, 1.8231892681797035e-05, 1.8097505744663067e-05, 1.7899765225593e-05, 1.7670379747869447e-05, 1.7347729226457886e-05, 1.719662941468414e-05, 1.7126989405369386e-05, 1.70568200701382e-05, 1.6870375475264154e-05, 1.6809521184768528e-05, 7.099055801518261e-05, 5.067719757789746e-05, 3.9829690649639815e-05, 3.325055877212435e-05, 2.9112772608641535e-05, 2.6318301024730317e-05, 2.445261452521663e-05, 2.308588409505319e-05, 2.2126438125269488e-05, 2.1344761989894323e-05, 2.0681814930867404e-05, 2.0177483747829683e-05, 1.989527663681656e-05, 1.9362389139132574e-05, 1.9019136743736453e-05, 1.87702571565751e-05, 1.8758539226837456e-05, 1.8457387341186404e-05, 1.8294467736268416e-05, 1.816580697777681e-05, 1.8029482816928066e-05, 1.7850221411208622e-05, 1.7776957975002006e-05, 1.7713909983285703e-05, 1.757348763931077e-05, 1.7512336853542365e-05, 1.751282070472371e-05, 1.7282722183153965e-05, 1.7069503883249126e-05, 1.7034182747011073e-05, 5.825610060128383e-05, 4.183177225058898e-05, 3.3203068596776575e-05, 2.8325470339041203e-05, 2.5291597921750508e-05, 2.3407732442137785e-05, 2.2124037059256807e-05, 2.122090336342808e-05, 2.052279160125181e-05, 1.9905337467207573e-05, 1.9522503862390295e-05, 1.9396547941141762e-05, 1.9060013073612936e-05, 1.8828808606485836e-05, 1.8626560631673783e-05, 1.8549053493188694e-05, 1.8441629435983486e-05, 1.834622526075691e-05, 1.8245131286676042e-05, 1.803732811822556e-05, 1.7999240299104713e-05, 1.792517286958173e-05, 1.774062548065558e-05, 1.7775470041669905e-05, 1.7726073565427214e-05, 1.7670819943305105e-05, 1.7533981008455157e-05, 1.7428688806830905e-05, 1.7386129911756143e-05, 1.74770102603361e-05, 4.861703928327188e-05, 3.5316727007739246e-05, 2.858717562048696e-05, 2.476392728567589e-05, 2.2589370928471908e-05, 2.120477074640803e-05, 2.034566023212392e-05, 1.9841287212329917e-05, 1.9269595213700086e-05, 1.8920653019449674e-05, 1.8749351511360146e-05, 1.8567203369457275e-05, 1.8441784050082788e-05, 1.825821163947694e-05, 1.8220769561594352e-05, 1.8226217434857972e-05, 1.808491288102232e-05, 1.80698207259411e-05, 1.7958485841518268e-05, 1.7891758034238592e-05, 1.7903272237163037e-05, 1.7907286746776663e-05, 1.7744101569405757e-05, 1.7817341358750127e-05, 1.7786227545002475e-05, 1.7706832295516506e-05, 1.7583741282578558e-05, 1.7553407815285027e-05, 1.7547408788232133e-05, 1.757809513946995e-05, 4.09793465223629e-05, 3.0257180696935393e-05, 2.4843468054314144e-05, 2.2059677576180547e-05, 2.0413002857821994e-05, 1.9506233002175577e-05, 1.887910366349388e-05, 1.8430389900458977e-05, 1.831711233535316e-05, 1.8166912923334166e-05, 1.7958565877052024e-05, 1.7867889255285263e-05, 1.7774793377611786e-05, 1.7696847862680443e-05, 1.7717014998197556e-05, 1.765563865774311e-05, 1.7739161194185726e-05, 1.7724709323374555e-05, 1.7647858840064146e-05, 1.7784092051442713e-05, 1.7726591977407224e-05, 1.7724212739267386e-05, 1.757436439220328e-05, 1.7593862139619887e-05, 1.7589316485100426e-05, 1.7602813386474736e-05, 1.754591903591063e-05, 1.7400021533831023e-05, 1.7360731362714432e-05, 1.7310081602772698e-05]
# training random data without spikes val = [5.1409765546850394e-06, 5.871886060049292e-06, 7.340346201090142e-06, 9.63521688390756e-06, 1.1888872904819436e-05, 1.3675818081537727e-05, 1.4842934433545452e-05, 1.5536093997070566e-05, 1.5920739315333776e-05, 1.6308365957229398e-05, 1.7124817532021552e-05, 1.730858457449358e-05, 1.761312159942463e-05, 1.7905991626321338e-05, 1.7738479073159397e-05]
#val random all training val = [8.044255991990212e-06, 1.5791070836712606e-05, 2.5500963602098636e-05, 2.8698466849164106e-05, 2.7995209165965207e-05, 2.835156374203507e-05, 2.8527316317195073e-05, 2.7957274141954258e-05, 2.7062324079452083e-05, 2.607553506095428e-05, 2.550070894358214e-05, 2.4829900212353095e-05, 2.4050847059697844e-05, 2.3465136109734885e-05, 2.3035583581076935e-05]
#val random all training bright val = [0.04727465659379959, 0.06290178745985031, 0.07326746731996536, 0.08017382025718689, 0.08749569207429886, 0.0960974395275116, 0.1041322648525238, 0.10900598019361496, 0.11385348439216614, 0.11792712658643723, 0.12129445374011993, 0.12432479858398438, 0.12634706497192383, 0.12808091938495636, 0.1316528022289276]